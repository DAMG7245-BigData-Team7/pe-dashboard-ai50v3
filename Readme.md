# Project ORBIT â€” PE Due Diligence Agentic System
## Agentification & MCP Integration for PE Dashboard (DAMG7245 - Assignment 5)

[![Tests](https://img.shields.io/badge/tests-37%2F37%20passing-brightgreen)](tests/)
[![Python](https://img.shields.io/badge/python-3.13-blue)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-blue)](LICENSE)

An intelligent, production-ready PE due diligence system powered by LangChain agents, LangGraph workflows, and the Model Context Protocol (MCP).

## ğŸ¯ Overview

Project ORBIT transforms traditional PE dashboard generation into an **agentic, automated workflow** with:

- ğŸ¤– **LangChain Agents**: Supervisor, Planner, and Evaluator agents with ReAct reasoning
- ğŸ”„ **LangGraph Workflows**: Graph-based orchestration with conditional branching
- ğŸŒ **MCP Server**: Standardized tool/resource/prompt exposure via HTTP
- ğŸš¨ **HITL Integration**: Human-in-the-loop approval for high-risk companies
- ğŸ“Š **Airflow DAGs**: Scheduled workflows for initial load, daily updates, and agentic dashboards
- ğŸ³ **Docker Deployment**: Full containerization with docker-compose

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph "Phase 1: Data Collection"
        A[Web Scraper] --> B[Raw Data]
        B --> C[LLM Extraction]
        C --> D[Structured Payloads]
        B --> E[Vector DB]
    end
    
    subgraph "Phase 2: Agentic Workflow"
        F[Airflow DAG] -->|Triggers| G[MCP Server]
        G --> H[Supervisor Agent]
        H --> I{Risk<br/>Detected?}
        I -->|Yes| J[HITL Approval]
        I -->|No| K[Auto-Approve]
        J --> L[Final Decision]
        K --> L
    end
    
    subgraph "Data Sources"
        D --> G
        E --> G
    end
    
    subgraph "Outputs"
        L --> M[Dashboards]
        L --> N[Risk Logs]
        H --> O[ReAct Traces]
    end
    
    style I fill:#ff6b6b,stroke:#c92a2a,color:#fff
    style J fill:#ff6b6b,stroke:#c92a2a,color:#fff
    style G fill:#339af0,stroke:#1971c2,color:#fff
    style H fill:#51cf66,stroke:#2f9e44,color:#fff
```

---

## âœ¨ Key Features

### Phase 1 - Agent Infrastructure (Labs 12-13) âœ…
- 3 core agent tools with async support
- Pydantic models for structured I/O
- Supervisor Agent with ReAct pattern
- Structured JSON logging with correlation IDs
- 10 unit tests (100% passing)

### Phase 2 - MCP Integration (Labs 14-15) âœ…
- MCP Server with 6 HTTP endpoints
- Dashboard generation tools (structured + RAG)
- Company resources and prompt templates
- Docker support with health checks
- 11 integration tests (100% passing)

### Phase 3 - Advanced Workflows (Labs 16-18) âœ…
- LangGraph StateGraph with 7 nodes
- Conditional branching (HITL vs Auto-Approve)
- Risk keyword detection (11 keywords)
- Interactive CLI pause/resume for HITL
- 16 workflow tests (100% passing)

### Phase 4 - Orchestration & Deployment âœ…
- 3 Airflow DAGs (initial load, daily update, agentic dashboard)
- Docker Compose for full stack deployment
- Configuration management (YAML + .env)
- Production-ready logging and monitoring

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.13+
- Docker & Docker Compose (optional)
- OpenAI API Key
- Pinecone API Key (optional)

### Option 1: Local Setup

```bash
# 1. Clone repository
git clone <your-repo-url>
cd pe-dashboard-ai50-v3

# 2. Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
# Edit .env with your API keys

# 5. Run MCP Server (Terminal 1)
 python -m src.server.mcp_server

# 6. Run Workflow (Terminal 2)
PYTHONPATH=. python3 src/workflows/due_diligence_graph.py anthropic

# 7. Run Tests
PYTHONPATH=. pytest -v
```

### Option 2: Docker Setup

```bash
# 1. Configure environment
cp .env.example .env
# Edit .env with your API keys

# 2. Start all services
docker-compose up --build

# 3. Check health
curl http://localhost:9000/health

# 4. Stop services
docker-compose down
```

---

## ğŸ“‹ Usage Examples

### Run Due Diligence Workflow

```bash
# Interactive mode (requires human approval for risky companies)
PYTHONPATH=. HITL_AUTO_APPROVE=false python3 src/workflows/due_diligence_graph.py anthropic

# Auto-approve mode (for automation)
PYTHONPATH=. HITL_AUTO_APPROVE=true python3 src/workflows/due_diligence_graph.py openai
```

### Call MCP Server Directly

```bash
# Health check
curl http://localhost:9000/health

# Get company list
curl http://localhost:9000/resource/ai50/companies

# Generate dashboard
curl -X POST http://localhost:9000/tool/generate_structured_dashboard \
  -H "Content-Type: application/json" \
  -d '{"company_id": "anthropic"}'
```

### Run Airflow DAGs

```bash
# Trigger initial load
airflow dags trigger orbit_initial_load

# Trigger agentic dashboard generation
airflow dags trigger orbit_agentic_dashboard

# Check status
airflow dags state orbit_agentic_dashboard <execution_date>
```

---

## ğŸ§ª Testing

```bash
# Run all tests
PYTHONPATH=. pytest -v

# Run specific test suite
PYTHONPATH=. pytest tests/test_tools.py -v
PYTHONPATH=. pytest tests/test_mcpserver.py -v
PYTHONPATH=. pytest tests/test_workflow_branches.py -v

# Run with coverage
PYTHONPATH=. pytest --cov=src --cov-report=html
```

**Test Results**: âœ… 37/37 tests passing (100%)

---

## ğŸ“ Project Structure

```
pe-dashboard-ai50-v3/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ supervisor_agent.py       # Main supervisor with ReAct
â”‚   â”‚   â”œâ”€â”€ planner_agent.py          # Plan generation
â”‚   â”‚   â””â”€â”€ evaluation_agent.py       # Dashboard evaluation
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ payload_tool.py           # Payload retrieval
â”‚   â”‚   â”œâ”€â”€ rag_tool.py               # Vector DB search
â”‚   â”‚   â””â”€â”€ risk_logger.py            # Risk signal logging
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â””â”€â”€ due_diligence_graph.py    # LangGraph workflow
â”‚   â”œâ”€â”€ server/
â”‚   â”‚   â””â”€â”€ mcp_server.py             # MCP HTTP server
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ react_logger.py           # ReAct trace logger
â”‚   â”‚   â””â”€â”€ dashboard_generator.py    # Dashboard generation
â”‚   â””â”€â”€ models.py                      # Pydantic models
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_tools.py                  # Tool unit tests
â”‚   â”œâ”€â”€ test_mcpserver.py              # MCP integration tests
â”‚   â””â”€â”€ test_workflow_branches.py      # Workflow branch tests
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ orbig_agentic_dashboard_dag.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ mcp_config.json                # MCP client config
â”‚   â””â”€â”€ settings_example.yaml          # Application settings
â”œâ”€â”€ data/                               # Runtime data
â”œâ”€â”€ logs/                               # Log files
â”œâ”€â”€ docker-compose.yml                 # Full stack orchestration
â”œâ”€â”€ .env.example                        # Environment template
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ Dockerfile.mcp                 
â”œâ”€â”€ Dockerfile.agent                
â”œâ”€â”€ Dockerfile.airflow                
â””â”€â”€ README.md                           # This file
```

---

## ğŸ”§ Configuration

### Environment Variables (.env)

```bash
# API Keys
OPENAI_API_KEY=your_key_here
PINECONE_API_KEY=your_key_here

# MCP Server
MCP_BASE_URL=http://localhost:9000
MCP_PORT=9000

# Agent Config
AGENT_MODEL=gpt-4o-mini
AGENT_TEMPERATURE=0

# HITL Config
HITL_AUTO_APPROVE=false  # Set to true for automation
```

### Application Settings (config/settings.yaml)

Copy `config/settings_example.yaml` to `config/settings.yaml` and customize:

- API endpoints and keys
- Workflow configuration
- Risk detection keywords
- Logging levels
- Performance tuning

---

## ğŸ“Š Workflow Execution

### No-Risk Path (Auto-Approve)

```
START â†’ Planner â†’ Data Generator â†’ Evaluator â†’ Risk Detector
        â†’ Auto-Approve â†’ Final Decision â†’ END

Duration: ~2-5 seconds
```

### Risk Path (HITL Required)

```
START â†’ Planner â†’ Data Generator â†’ Evaluator â†’ Risk Detector
        â†’ HITL (Human Approval) â†’ Final Decision â†’ END

Duration: Variable (depends on human response time)
```

---

## ğŸ› Troubleshooting

### MCP Server Not Starting

```bash
# Check if port 9000 is in use
lsof -i :9000

# Kill process using port
kill -9 <PID>

# Restart MCP server
uvicorn src.server.mcp_server:app --port 9000
```

### Workflow Hangs During HITL

```bash
# Set auto-approve mode
export HITL_AUTO_APPROVE=true

# Or run with environment variable
HITL_AUTO_APPROVE=true python3 src/workflows/due_diligence_graph.py anthropic
```

### Tests Failing

```bash
# Ensure PYTHONPATH is set
export PYTHONPATH=.

# Set HITL auto-approve for tests
export HITL_AUTO_APPROVE=true

# Run tests
pytest -v
```

---

## ğŸ“ˆ Metrics & Performance

- **Total Lines of Code**: ~5,700 lines
- **Test Coverage**: 37 tests, 100% passing
- **MCP Endpoints**: 6 endpoints
- **Workflow Nodes**: 7 nodes
- **Risk Keywords**: 11 monitored keywords
- **Airflow DAGs**: 3 DAGs

---


## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for detail

---

## ğŸ‘¥ Team

- **Course**: DAMG7245 - Big Data Systems & Intelligence Analytics
- **Assignment**: Assignment 5 - Project ORBIT Part 2
- **Institution**: Northeastern University

---

## Acknowledgments

- LangChain & LangGraph for agent frameworks
- Model Context Protocol (MCP) specification
- Apache Airflow for orchestration
- OpenAI & Pinecone for AI infrastructure

---


**Last Updated**: November 16, 2025